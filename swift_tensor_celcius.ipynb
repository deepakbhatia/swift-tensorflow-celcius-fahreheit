{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "swift_tensor_celcius.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hd7cjYx2vrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import TensorFlow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s__imoO8LdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import PythonKit\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNvA9JQQ25XS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let hiddenSize: Int = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na4l2x012-Js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let np = Python.import(\"numpy\")\n",
        "\n",
        "let array = np.arange(100).reshape(10, 10)  // Create a 10x10 numpy array.\n",
        "let tensor = Tensor<Float>(numpy: array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYMbJvsQuHhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func cToF(from celcius: Float) -> Float {\n",
        "  return (celcius/5) * (9) + 32\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVqnPHP68fdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let cel: [Float] = [14, 25, 12, 34, 22, 10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anwtyWiMNh6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let celSwT = Tensor<Float>(shape: [cel.count, 1] , scalars: cel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkoK8IB0N2ql",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d220843f-1c39-46d6-f1de-e90b875fefa6"
      },
      "source": [
        "print(cel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14.0, 25.0, 12.0, 34.0, 22.0, 10.0]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwYv8D2yN4iR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "9ab9617c-9ddf-4fe7-fe3e-63a02cd6548b"
      },
      "source": [
        "print(celSwT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14.0],\r\n",
            " [25.0],\r\n",
            " [12.0],\r\n",
            " [34.0],\r\n",
            " [22.0],\r\n",
            " [10.0]]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrC6HXitOIlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let fah: [Float] = cel.map(cToF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iik4QJMuOVdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let fahSet = Tensor(shape: [fah.count, 1], scalars: fah)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE-bPJEYu-cp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "5a47fa72-3adb-4042-a2cd-986214141feb"
      },
      "source": [
        "print(fahSet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[57.199997],\r\n",
            " [     77.0],\r\n",
            " [     53.6],\r\n",
            " [     93.2],\r\n",
            " [71.600006],\r\n",
            " [     50.0]]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_ggJzucwEjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "struct CFModel: Layer {\n",
        "  var neuron = Dense<Float>(inputSize: 1, outputSize: 1)\n",
        "\n",
        "  @differentiable\n",
        "  func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "    neuron(input)\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUAyg1-kxCkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var model = CFModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFipR8uExvPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Context.local.learningPhase = .training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h84fy22Ix16f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let optimizer = Adam(for: model, learningRate: 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v1Kjear1PuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let epochCount = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OZmUUxS1W3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var trainingLoss: [Float] = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EwfU21y1tvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOF8mGwIxxTK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b554bee2-d803-4fd8-ed23-20cfcc5a2dad"
      },
      "source": [
        "(1...epochCount).forEach { epoch in \n",
        "  let (loss, grad) =  valueWithGradient(at: model) { model -> Tensor<Float> in\n",
        "    let prediction = model(celSwT)\n",
        "    return meanSquaredError(predicted: prediction, expected: fahSet)\n",
        "  }\n",
        "  optimizer.update(&model, along: grad)\n",
        "  let epochLoss = loss.scalarized()\n",
        "  trainingLoss.append(epochLoss)\n",
        "  if Bool.random() {\n",
        "    print(\"Epoch \\(epoch): Loss: \\(loss)\")\n",
        "  }\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss: 40.771893\r\n",
            "Epoch 4: Loss: 40.72575\r\n",
            "Epoch 7: Loss: 40.587643\r\n",
            "Epoch 8: Loss: 40.541645\r\n",
            "Epoch 9: Loss: 40.49573\r\n",
            "Epoch 10: Loss: 40.449863\r\n",
            "Epoch 12: Loss: 40.358196\r\n",
            "Epoch 13: Loss: 40.31244\r\n",
            "Epoch 14: Loss: 40.26671\r\n",
            "Epoch 15: Loss: 40.221004\r\n",
            "Epoch 16: Loss: 40.175343\r\n",
            "Epoch 18: Loss: 40.08417\r\n",
            "Epoch 24: Loss: 39.811512\r\n",
            "Epoch 25: Loss: 39.7662\r\n",
            "Epoch 26: Loss: 39.720913\r\n",
            "Epoch 27: Loss: 39.67569\r\n",
            "Epoch 28: Loss: 39.630497\r\n",
            "Epoch 30: Loss: 39.54025\r\n",
            "Epoch 31: Loss: 39.49516\r\n",
            "Epoch 32: Loss: 39.4501\r\n",
            "Epoch 33: Loss: 39.40511\r\n",
            "Epoch 36: Loss: 39.270336\r\n",
            "Epoch 37: Loss: 39.22547\r\n",
            "Epoch 38: Loss: 39.18068\r\n",
            "Epoch 39: Loss: 39.13589\r\n",
            "Epoch 41: Loss: 39.04646\r\n",
            "Epoch 42: Loss: 39.001804\r\n",
            "Epoch 44: Loss: 38.912613\r\n",
            "Epoch 46: Loss: 38.82356\r\n",
            "Epoch 47: Loss: 38.77908\r\n",
            "Epoch 48: Loss: 38.73465\r\n",
            "Epoch 49: Loss: 38.690258\r\n",
            "Epoch 51: Loss: 38.60159\r\n",
            "Epoch 53: Loss: 38.513042\r\n",
            "Epoch 54: Loss: 38.468853\r\n",
            "Epoch 55: Loss: 38.424694\r\n",
            "Epoch 56: Loss: 38.380543\r\n",
            "Epoch 58: Loss: 38.29239\r\n",
            "Epoch 61: Loss: 38.16048\r\n",
            "Epoch 62: Loss: 38.116558\n",
            "Epoch 65: Loss: 37.98508\n",
            "Epoch 69: Loss: 37.81029\n",
            "Epoch 71: Loss: 37.723125\n",
            "Epoch 73: Loss: 37.63611\n",
            "Epoch 75: Loss: 37.549225\n",
            "Epoch 76: Loss: 37.505863\n",
            "Epoch 77: Loss: 37.462524\n",
            "Epoch 78: Loss: 37.419216\n",
            "Epoch 79: Loss: 37.375946\n",
            "Epoch 80: Loss: 37.332737\n",
            "Epoch 85: Loss: 37.117123\n",
            "Epoch 87: Loss: 37.031136\n",
            "Epoch 90: Loss: 36.902447\n",
            "Epoch 91: Loss: 36.859623\n",
            "Epoch 93: Loss: 36.774105\n",
            "Epoch 95: Loss: 36.688694\n",
            "Epoch 96: Loss: 36.64606\n",
            "Epoch 97: Loss: 36.60346\n",
            "Epoch 104: Loss: 36.30629\n",
            "Epoch 106: Loss: 36.221714\n",
            "Epoch 109: Loss: 36.095104\n",
            "Epoch 110: Loss: 36.05299\n",
            "Epoch 118: Loss: 35.7173\n",
            "Epoch 119: Loss: 35.675495\n",
            "Epoch 120: Loss: 35.63374\n",
            "Epoch 121: Loss: 35.592026\n",
            "Epoch 122: Loss: 35.550343\n",
            "Epoch 123: Loss: 35.50869\n",
            "Epoch 124: Loss: 35.46706\n",
            "Epoch 125: Loss: 35.425495\n",
            "Epoch 127: Loss: 35.342453\n",
            "Epoch 128: Loss: 35.30098\n",
            "Epoch 130: Loss: 35.21815\n",
            "Epoch 132: Loss: 35.13548\n",
            "Epoch 133: Loss: 35.094177\n",
            "Epoch 135: Loss: 35.011692\n",
            "Epoch 136: Loss: 34.970524\n",
            "Epoch 137: Loss: 34.929375\n",
            "Epoch 138: Loss: 34.888252\n",
            "Epoch 142: Loss: 34.724186\n",
            "Epoch 143: Loss: 34.683247\n",
            "Epoch 144: Loss: 34.64238\n",
            "Epoch 146: Loss: 34.560684\n",
            "Epoch 150: Loss: 34.397747\n",
            "Epoch 151: Loss: 34.35711\n",
            "Epoch 152: Loss: 34.316494\n",
            "Epoch 153: Loss: 34.275932\n",
            "Epoch 154: Loss: 34.2354\n",
            "Epoch 155: Loss: 34.1949\n",
            "Epoch 157: Loss: 34.11398\n",
            "Epoch 160: Loss: 33.9929\n",
            "Epoch 161: Loss: 33.952633\n",
            "Epoch 164: Loss: 33.831974\n",
            "Epoch 165: Loss: 33.79182\n",
            "Epoch 166: Loss: 33.751713\n",
            "Epoch 168: Loss: 33.671604\n",
            "Epoch 169: Loss: 33.63161\n",
            "Epoch 170: Loss: 33.59164\n",
            "Epoch 171: Loss: 33.55171\n",
            "Epoch 172: Loss: 33.5118\n",
            "Epoch 173: Loss: 33.471954\n",
            "Epoch 174: Loss: 33.432125\n",
            "Epoch 177: Loss: 33.31284\n",
            "Epoch 179: Loss: 33.233494\n",
            "Epoch 180: Loss: 33.193897\n",
            "Epoch 182: Loss: 33.114773\n",
            "Epoch 183: Loss: 33.07523\n",
            "Epoch 184: Loss: 33.035778\n",
            "Epoch 185: Loss: 32.996346\n",
            "Epoch 186: Loss: 32.956924\n",
            "Epoch 187: Loss: 32.917534\n",
            "Epoch 188: Loss: 32.87822\n",
            "Epoch 189: Loss: 32.83891\n",
            "Epoch 196: Loss: 32.564766\n",
            "Epoch 198: Loss: 32.486782\n",
            "Epoch 200: Loss: 32.408886\n",
            "Epoch 201: Loss: 32.37001\n",
            "Epoch 203: Loss: 32.292347\n",
            "Epoch 205: Loss: 32.214832\n",
            "Epoch 206: Loss: 32.17612\n",
            "Epoch 208: Loss: 32.098785\n",
            "Epoch 209: Loss: 32.060192\n",
            "Epoch 210: Loss: 32.021614\n",
            "Epoch 211: Loss: 31.983063\n",
            "Epoch 212: Loss: 31.944582\n",
            "Epoch 213: Loss: 31.9061\n",
            "Epoch 216: Loss: 31.790895\n",
            "Epoch 217: Loss: 31.752584\n",
            "Epoch 220: Loss: 31.63778\n",
            "Epoch 222: Loss: 31.56144\n",
            "Epoch 223: Loss: 31.523306\n",
            "Epoch 224: Loss: 31.48523\n",
            "Epoch 227: Loss: 31.371162\n",
            "Epoch 229: Loss: 31.29527\n",
            "Epoch 230: Loss: 31.257385\n",
            "Epoch 231: Loss: 31.219538\n",
            "Epoch 232: Loss: 31.181702\n",
            "Epoch 234: Loss: 31.106169\n",
            "Epoch 235: Loss: 31.068445\n",
            "Epoch 238: Loss: 30.955511\n",
            "Epoch 239: Loss: 30.917933\n",
            "Epoch 240: Loss: 30.880373\n",
            "Epoch 243: Loss: 30.76795\n",
            "Epoch 244: Loss: 30.73053\n",
            "Epoch 246: Loss: 30.655817\n",
            "Epoch 247: Loss: 30.618492\n",
            "Epoch 248: Loss: 30.58124\n",
            "Epoch 249: Loss: 30.543985\n",
            "Epoch 251: Loss: 30.469595\n",
            "Epoch 252: Loss: 30.432474\n",
            "Epoch 254: Loss: 30.358267\n",
            "Epoch 255: Loss: 30.321238\n",
            "Epoch 256: Loss: 30.284231\n",
            "Epoch 258: Loss: 30.210333\n",
            "Epoch 260: Loss: 30.13652\n",
            "Epoch 262: Loss: 30.062883\n",
            "Epoch 266: Loss: 29.916\n",
            "Epoch 269: Loss: 29.806192\n",
            "Epoch 270: Loss: 29.769648\n",
            "Epoch 272: Loss: 29.696655\n",
            "Epoch 273: Loss: 29.660227\n",
            "Epoch 275: Loss: 29.587448\n",
            "Epoch 277: Loss: 29.514807\n",
            "Epoch 278: Loss: 29.478533\n",
            "Epoch 280: Loss: 29.406075\n",
            "Epoch 281: Loss: 29.369905\n",
            "Epoch 282: Loss: 29.33377\n",
            "Epoch 283: Loss: 29.297646\n",
            "Epoch 286: Loss: 29.189539\n",
            "Epoch 290: Loss: 29.045809\n",
            "Epoch 292: Loss: 28.974161\n",
            "Epoch 293: Loss: 28.938387\n",
            "Epoch 296: Loss: 28.831263\n",
            "Epoch 297: Loss: 28.795603\n",
            "Epoch 298: Loss: 28.75998\n",
            "Epoch 300: Loss: 28.688852\n",
            "Epoch 301: Loss: 28.653353\n",
            "Epoch 302: Loss: 28.617859\n",
            "Epoch 306: Loss: 28.476255\n",
            "Epoch 311: Loss: 28.299982\n",
            "Epoch 312: Loss: 28.264832\n",
            "Epoch 313: Loss: 28.229698\n",
            "Epoch 314: Loss: 28.194603\n",
            "Epoch 319: Loss: 28.019613\n",
            "Epoch 320: Loss: 27.984726\n",
            "Epoch 321: Loss: 27.949852\n",
            "Epoch 322: Loss: 27.915041\n",
            "Epoch 323: Loss: 27.880217\n",
            "Epoch 325: Loss: 27.81073\n",
            "Epoch 328: Loss: 27.706705\n",
            "Epoch 333: Loss: 27.533983\n",
            "Epoch 334: Loss: 27.499514\n",
            "Epoch 335: Loss: 27.465124\n",
            "Epoch 337: Loss: 27.396368\n",
            "Epoch 341: Loss: 27.259306\n",
            "Epoch 342: Loss: 27.225111\n",
            "Epoch 344: Loss: 27.156809\n",
            "Epoch 348: Loss: 27.020617\n",
            "Epoch 350: Loss: 26.952696\n",
            "Epoch 351: Loss: 26.9188\n",
            "Epoch 354: Loss: 26.817282\n",
            "Epoch 357: Loss: 26.716024\n",
            "Epoch 358: Loss: 26.682348\n",
            "Epoch 360: Loss: 26.61506\n",
            "Epoch 366: Loss: 26.413994\n",
            "Epoch 367: Loss: 26.380598\n",
            "Epoch 369: Loss: 26.313887\n",
            "Epoch 370: Loss: 26.280563\n",
            "Epoch 371: Loss: 26.247305\n",
            "Epoch 372: Loss: 26.21406\n",
            "Epoch 374: Loss: 26.147669\n",
            "Epoch 375: Loss: 26.114517\n",
            "Epoch 376: Loss: 26.081404\n",
            "Epoch 377: Loss: 26.04833\n",
            "Epoch 379: Loss: 25.982222\n",
            "Epoch 380: Loss: 25.94925\n",
            "Epoch 382: Loss: 25.883339\n",
            "Epoch 385: Loss: 25.78475\n",
            "Epoch 389: Loss: 25.653696\n",
            "Epoch 391: Loss: 25.588366\n",
            "Epoch 394: Loss: 25.490583\n",
            "Epoch 395: Loss: 25.458038\n",
            "Epoch 397: Loss: 25.393082\n",
            "Epoch 398: Loss: 25.360651\n",
            "Epoch 400: Loss: 25.295876\n",
            "Epoch 401: Loss: 25.263544\n",
            "Epoch 403: Loss: 25.198923\n",
            "Epoch 404: Loss: 25.166677\n",
            "Epoch 405: Loss: 25.13445\n",
            "Epoch 408: Loss: 25.03797\n",
            "Epoch 410: Loss: 24.973814\n",
            "Epoch 412: Loss: 24.909773\n",
            "Epoch 414: Loss: 24.845846\n",
            "Epoch 415: Loss: 24.813936\n",
            "Epoch 418: Loss: 24.718363\n",
            "Epoch 421: Loss: 24.623064\n",
            "Epoch 427: Loss: 24.433325\n",
            "Epoch 428: Loss: 24.40181\n",
            "Epoch 429: Loss: 24.370295\n",
            "Epoch 431: Loss: 24.307419\n",
            "Epoch 432: Loss: 24.276022\n",
            "Epoch 434: Loss: 24.21332\n",
            "Epoch 437: Loss: 24.119486\n",
            "Epoch 438: Loss: 24.088257\n",
            "Epoch 440: Loss: 24.025917\n",
            "Epoch 441: Loss: 23.994787\n",
            "Epoch 442: Loss: 23.96369\n",
            "Epoch 443: Loss: 23.932615\n",
            "Epoch 445: Loss: 23.870575\n",
            "Epoch 448: Loss: 23.77774\n",
            "Epoch 449: Loss: 23.746841\n",
            "Epoch 451: Loss: 23.685179\n",
            "Epoch 453: Loss: 23.623583\n",
            "Epoch 454: Loss: 23.592855\n",
            "Epoch 464: Loss: 23.287104\n",
            "Epoch 465: Loss: 23.256681\n",
            "Epoch 466: Loss: 23.226303\n",
            "Epoch 468: Loss: 23.165636\n",
            "Epoch 473: Loss: 23.01446\n",
            "Epoch 474: Loss: 22.984308\n",
            "Epoch 476: Loss: 22.924109\n",
            "Epoch 477: Loss: 22.894043\n",
            "Epoch 479: Loss: 22.83403\n",
            "Epoch 480: Loss: 22.804047\n",
            "Epoch 481: Loss: 22.774115\n",
            "Epoch 482: Loss: 22.744188\n",
            "Epoch 483: Loss: 22.714302\n",
            "Epoch 484: Loss: 22.68445\n",
            "Epoch 485: Loss: 22.654638\n",
            "Epoch 486: Loss: 22.624832\n",
            "Epoch 487: Loss: 22.59506\n",
            "Epoch 488: Loss: 22.565329\n",
            "Epoch 491: Loss: 22.476282\n",
            "Epoch 494: Loss: 22.387497\n",
            "Epoch 497: Loss: 22.29899\n",
            "Epoch 500: Loss: 22.210747\n",
            "Epoch 504: Loss: 22.093475\n",
            "Epoch 510: Loss: 21.91845\n",
            "Epoch 513: Loss: 21.831312\n",
            "Epoch 520: Loss: 21.629\n",
            "Epoch 521: Loss: 21.600233\n",
            "Epoch 522: Loss: 21.571459\n",
            "Epoch 524: Loss: 21.514036\n",
            "Epoch 528: Loss: 21.399519\n",
            "Epoch 530: Loss: 21.34246\n",
            "Epoch 531: Loss: 21.313932\n",
            "Epoch 534: Loss: 21.228601\n",
            "Epoch 537: Loss: 21.143522\n",
            "Epoch 540: Loss: 21.058702\n",
            "Epoch 544: Loss: 20.945986\n",
            "Epoch 547: Loss: 20.861763\n",
            "Epoch 549: Loss: 20.805742\n",
            "Epoch 550: Loss: 20.777784\n",
            "Epoch 554: Loss: 20.666178\n",
            "Epoch 556: Loss: 20.610567\n",
            "Epoch 557: Loss: 20.5828\n",
            "Epoch 558: Loss: 20.555061\n",
            "Epoch 560: Loss: 20.499674\n",
            "Epoch 562: Loss: 20.444353\n",
            "Epoch 563: Loss: 20.416758\n",
            "Epoch 564: Loss: 20.389189\n",
            "Epoch 565: Loss: 20.36164\n",
            "Epoch 566: Loss: 20.33412\n",
            "Epoch 570: Loss: 20.224306\n",
            "Epoch 571: Loss: 20.196932\n",
            "Epoch 573: Loss: 20.142242\n",
            "Epoch 574: Loss: 20.114935\n",
            "Epoch 575: Loss: 20.087677\n",
            "Epoch 576: Loss: 20.060434\n",
            "Epoch 577: Loss: 20.033216\n",
            "Epoch 578: Loss: 20.006025\n",
            "Epoch 579: Loss: 19.978876\n",
            "Epoch 580: Loss: 19.951725\n",
            "Epoch 583: Loss: 19.870516\n",
            "Epoch 584: Loss: 19.843477\n",
            "Epoch 586: Loss: 19.78951\n",
            "Epoch 587: Loss: 19.762556\n",
            "Epoch 589: Loss: 19.708769\n",
            "Epoch 592: Loss: 19.628267\n",
            "Epoch 593: Loss: 19.601505\n",
            "Epoch 594: Loss: 19.574759\n",
            "Epoch 596: Loss: 19.52133\n",
            "Epoch 597: Loss: 19.494658\n",
            "Epoch 598: Loss: 19.468016\n",
            "Epoch 599: Loss: 19.441393\n",
            "Epoch 600: Loss: 19.414818\n",
            "Epoch 601: Loss: 19.388254\n",
            "Epoch 604: Loss: 19.30874\n",
            "Epoch 605: Loss: 19.282286\n",
            "Epoch 607: Loss: 19.229456\n",
            "Epoch 610: Loss: 19.150427\n",
            "Epoch 612: Loss: 19.097864\n",
            "Epoch 615: Loss: 19.019249\n",
            "Epoch 618: Loss: 18.940851\n",
            "Epoch 622: Loss: 18.836721\n",
            "Epoch 624: Loss: 18.784807\n",
            "Epoch 627: Loss: 18.707151\n",
            "Epoch 628: Loss: 18.68131\n",
            "Epoch 631: Loss: 18.603971\n",
            "Epoch 634: Loss: 18.526855\n",
            "Epoch 636: Loss: 18.475588\n",
            "Epoch 638: Loss: 18.424421\n",
            "Epoch 641: Loss: 18.347878\n",
            "Epoch 643: Loss: 18.296974\n",
            "Epoch 645: Loss: 18.246183\n",
            "Epoch 647: Loss: 18.195494\n",
            "Epoch 648: Loss: 18.170183\n",
            "Epoch 649: Loss: 18.144905\n",
            "Epoch 650: Loss: 18.119663\n",
            "Epoch 652: Loss: 18.069231\n",
            "Epoch 653: Loss: 18.04405\n",
            "Epoch 654: Loss: 18.018906\n",
            "Epoch 655: Loss: 17.993782\n",
            "Epoch 660: Loss: 17.868574\n",
            "Epoch 661: Loss: 17.843597\n",
            "Epoch 665: Loss: 17.744003\n",
            "Epoch 670: Loss: 17.620085\n",
            "Epoch 672: Loss: 17.570696\n",
            "Epoch 673: Loss: 17.546038\n",
            "Epoch 676: Loss: 17.472223\n",
            "Epoch 679: Loss: 17.39866\n",
            "Epoch 680: Loss: 17.374174\n",
            "Epoch 686: Loss: 17.22789\n",
            "Epoch 687: Loss: 17.203592\n",
            "Epoch 688: Loss: 17.179327\n",
            "Epoch 689: Loss: 17.155083\n",
            "Epoch 690: Loss: 17.130865\n",
            "Epoch 692: Loss: 17.082506\n",
            "Epoch 693: Loss: 17.058365\n",
            "Epoch 695: Loss: 17.010162\n",
            "Epoch 698: Loss: 16.938042\n",
            "Epoch 701: Loss: 16.866167\n",
            "Epoch 702: Loss: 16.842262\n",
            "Epoch 704: Loss: 16.794504\n",
            "Epoch 705: Loss: 16.77067\n",
            "Epoch 711: Loss: 16.628172\n",
            "Epoch 714: Loss: 16.557295\n",
            "Epoch 718: Loss: 16.463114\n",
            "Epoch 721: Loss: 16.392735\n",
            "Epoch 724: Loss: 16.32258\n",
            "Epoch 726: Loss: 16.275923\n",
            "Epoch 733: Loss: 16.11348\n",
            "Epoch 734: Loss: 16.090368\n",
            "Epoch 736: Loss: 16.044224\n",
            "Epoch 737: Loss: 16.021193\n",
            "Epoch 738: Loss: 15.998185\n",
            "Epoch 739: Loss: 15.975193\n",
            "Epoch 740: Loss: 15.95224\n",
            "Epoch 742: Loss: 15.906388\n",
            "Epoch 743: Loss: 15.883502\n",
            "Epoch 744: Loss: 15.860646\n",
            "Epoch 746: Loss: 15.814995\n",
            "Epoch 747: Loss: 15.792203\n",
            "Epoch 750: Loss: 15.723979\n",
            "Epoch 751: Loss: 15.701291\n",
            "Epoch 753: Loss: 15.65599\n",
            "Epoch 754: Loss: 15.633361\n",
            "Epoch 755: Loss: 15.610784\n",
            "Epoch 756: Loss: 15.588203\n",
            "Epoch 758: Loss: 15.543157\n",
            "Epoch 759: Loss: 15.520673\n",
            "Epoch 760: Loss: 15.498199\n",
            "Epoch 763: Loss: 15.430942\n",
            "Epoch 765: Loss: 15.386199\n",
            "Epoch 766: Loss: 15.363894\n",
            "Epoch 769: Loss: 15.297058\n",
            "Epoch 771: Loss: 15.25262\n",
            "Epoch 773: Loss: 15.208299\n",
            "Epoch 774: Loss: 15.186164\n",
            "Epoch 775: Loss: 15.164059\n",
            "Epoch 778: Loss: 15.097888\n",
            "Epoch 779: Loss: 15.075864\n",
            "Epoch 780: Loss: 15.053889\n",
            "Epoch 781: Loss: 15.031928\n",
            "Epoch 782: Loss: 15.009987\n",
            "Epoch 784: Loss: 14.966182\n",
            "Epoch 786: Loss: 14.922462\n",
            "Epoch 792: Loss: 14.791921\n",
            "Epoch 794: Loss: 14.748599\n",
            "Epoch 795: Loss: 14.726964\n",
            "Epoch 796: Loss: 14.705368\n",
            "Epoch 797: Loss: 14.683795\n",
            "Epoch 798: Loss: 14.662242\n",
            "Epoch 799: Loss: 14.640704\n",
            "Epoch 800: Loss: 14.619186\n",
            "Epoch 801: Loss: 14.597706\n",
            "Epoch 806: Loss: 14.49064\n",
            "Epoch 807: Loss: 14.469296\n",
            "Epoch 809: Loss: 14.426679\n",
            "Epoch 810: Loss: 14.405403\n",
            "Epoch 811: Loss: 14.384155\n",
            "Epoch 812: Loss: 14.362942\n",
            "Epoch 814: Loss: 14.320563\n",
            "Epoch 815: Loss: 14.299405\n",
            "Epoch 816: Loss: 14.278267\n",
            "Epoch 818: Loss: 14.236099\n",
            "Epoch 824: Loss: 14.110374\n",
            "Epoch 827: Loss: 14.048083\n",
            "Epoch 829: Loss: 14.0061035\n",
            "Epoch 831: Loss: 13.964089\n",
            "Epoch 832: Loss: 13.943398\n",
            "Epoch 833: Loss: 13.922806\n",
            "Epoch 834: Loss: 13.902207\n",
            "Epoch 835: Loss: 13.881439\n",
            "Epoch 839: Loss: 13.798779\n",
            "Epoch 844: Loss: 13.69604\n",
            "Epoch 845: Loss: 13.675647\n",
            "Epoch 846: Loss: 13.655216\n",
            "Epoch 848: Loss: 13.614334\n",
            "Epoch 849: Loss: 13.59396\n",
            "Epoch 851: Loss: 13.553341\n",
            "Epoch 852: Loss: 13.533038\n",
            "Epoch 853: Loss: 13.512745\n",
            "Epoch 855: Loss: 13.47224\n",
            "Epoch 860: Loss: 13.371446\n",
            "Epoch 861: Loss: 13.351341\n",
            "Epoch 865: Loss: 13.27119\n",
            "Epoch 866: Loss: 13.251203\n",
            "Epoch 872: Loss: 13.131795\n",
            "Epoch 873: Loss: 13.111995\n",
            "Epoch 874: Loss: 13.092193\n",
            "Epoch 875: Loss: 13.072411\n",
            "Epoch 876: Loss: 13.052663\n",
            "Epoch 878: Loss: 13.013194\n",
            "Epoch 879: Loss: 12.993531\n",
            "Epoch 880: Loss: 12.973862\n",
            "Epoch 881: Loss: 12.954204\n",
            "Epoch 883: Loss: 12.914987\n",
            "Epoch 884: Loss: 12.895409\n",
            "Epoch 886: Loss: 12.85632\n",
            "Epoch 888: Loss: 12.81733\n",
            "Epoch 891: Loss: 12.759006\n",
            "Epoch 892: Loss: 12.739587\n",
            "Epoch 893: Loss: 12.720226\n",
            "Epoch 896: Loss: 12.662224\n",
            "Epoch 901: Loss: 12.566006\n",
            "Epoch 904: Loss: 12.508555\n",
            "Epoch 909: Loss: 12.413217\n",
            "Epoch 912: Loss: 12.356267\n",
            "Epoch 913: Loss: 12.337337\n",
            "Epoch 916: Loss: 12.280656\n",
            "Epoch 920: Loss: 12.205395\n",
            "Epoch 921: Loss: 12.1866255\n",
            "Epoch 923: Loss: 12.149178\n",
            "Epoch 925: Loss: 12.111796\n",
            "Epoch 928: Loss: 12.055916\n",
            "Epoch 929: Loss: 12.037322\n",
            "Epoch 931: Loss: 12.000209\n",
            "Epoch 933: Loss: 11.963176\n",
            "Epoch 934: Loss: 11.9446945\n",
            "Epoch 935: Loss: 11.926236\n",
            "Epoch 936: Loss: 11.907792\n",
            "Epoch 937: Loss: 11.889381\n",
            "Epoch 938: Loss: 11.870998\n",
            "Epoch 940: Loss: 11.834255\n",
            "Epoch 941: Loss: 11.815934\n",
            "Epoch 943: Loss: 11.779338\n",
            "Epoch 944: Loss: 11.761078\n",
            "Epoch 947: Loss: 11.70639\n",
            "Epoch 948: Loss: 11.6882105\n",
            "Epoch 949: Loss: 11.670054\n",
            "Epoch 951: Loss: 11.633794\n",
            "Epoch 953: Loss: 11.5976305\n",
            "Epoch 955: Loss: 11.561531\n",
            "Epoch 956: Loss: 11.543519\n",
            "Epoch 957: Loss: 11.525527\n",
            "Epoch 964: Loss: 11.400197\n",
            "Epoch 965: Loss: 11.382375\n",
            "Epoch 968: Loss: 11.32904\n",
            "Epoch 969: Loss: 11.311292\n",
            "Epoch 971: Loss: 11.275876\n",
            "Epoch 973: Loss: 11.240551\n",
            "Epoch 976: Loss: 11.187707\n",
            "Epoch 977: Loss: 11.170143\n",
            "Epoch 978: Loss: 11.1525955\n",
            "Epoch 980: Loss: 11.11756\n",
            "Epoch 981: Loss: 11.100062\n",
            "Epoch 983: Loss: 11.065148\n",
            "Epoch 985: Loss: 11.0303335\n",
            "Epoch 986: Loss: 11.012942\n",
            "Epoch 990: Loss: 10.943612\n",
            "Epoch 993: Loss: 10.891841\n",
            "Epoch 994: Loss: 10.874619\n",
            "Epoch 996: Loss: 10.840247\n",
            "Epoch 997: Loss: 10.82309\n",
            "Epoch 998: Loss: 10.805954\n",
            "Epoch 999: Loss: 10.788844\n",
            "Epoch 1000: Loss: 10.771755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjsP6y8k_3ew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "3c93dcd8-ecd7-432e-9e4c-66af0c10c1ed"
      },
      "source": [
        "print(\"weight: \\(model.neuron.weight)\")\n",
        "print(\"bias: \\(model.neuron.bias)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight: [[2.1581964]]\r\n",
            "bias: [23.711895]\r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}